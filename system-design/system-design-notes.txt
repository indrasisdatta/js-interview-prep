System design tutorial: https://www.geeksforgeeks.org/system-design-tutorial/

Functional requirements (What the system should do to meet user needs - user authentication, data processing, user actions)
Non-functional requirements (How the system should perform - scalability, security, response time, reliability)

Components of System design:
 - Load balancer (ELB)
 - Caching (Redis, Memcache)
 - CDN (Cloudfront)
 - API Gateway (AWS API gateway)
 - Key value store (AWS DynamoDB)
 - Blob (Object) storage and database (AWS S3)
 - Rate limiter (AWS API gateway throttling)
 - Monitoring system (CloudWatch)
 - Distributed system messaging queue (SQS)
 - Distributed unique id generator (DynamoDB with UUID generation)
 - Distributed search/Elastic search - AWS Opensearch
 - Distributed logging service (CouldTrail, CloudWatch logs)
 - Distributed task scheduler (SNS)

Horizontal scaling/scale-out (add more servers to handle increased workload)
Vertical scaling/scale up (adding more CPU cores, memory or storage capacity to handle increased workload)

Load balancer - distribute the incoming traffic among servers to provide high availability, efficient server utilization and high performance.

Latency - time taken for a single task to complete (Request time + Response time) 
Throughput - No. of tasks completed in a given time period (High throughput usually means low latency)

Improve latency:
 - Deploy services closed to users (using CDN)
 - Load balancer and horizontal scaling
 - Optimize DB queries and index DB tables
 - Use caching Redis/Memcache
 - Async processing (handle non-critical tasks in the background eg. email notification in Amazon SQS)
 - DB sharding

Improve throughput: All above works, in addition to these: 
- Batch processing (Eg. instead of updating 100 transactions, 1 single query is fired instead of 100)
- Replication (use DB replica for read-heavy operations)

Database Sharding (eg. 4M rows -> 4 shards with 1M rows each)
- horizontal scaling of DB where data is split across multiple DB instances or shards 
Types of sharding:
 - Key based sharding (serverIndex = hashValue % numOfServers)
 - Range based sharding (customer name A-P in shard 1 and the rest in shard 2)
 - Vertical sharding (split multiple columns in single table to multiple tables)
    Eg. users (id, email, name, created_at) -> users (id, email, name) user_shard2 (user_id, created_at)
 - Directory based sharding (maintain lookup table for each shard)

Serverless - if traffic goes up and down a lot, serverless is cost-efficient by using resources only when needed

Event streaming (High volume real time data streams eg. Amazon Kinesis)
Message broker (Communication b/w distributed system producers and consumers eg. Amazon SQS)

API gateway
 - API composition (this layer fetches and combines data from multiple microservices 
   eg. when product details is called, data is aggregated from product service, review service, price service)
 - Authentication (Identity provider)
 - Rate limiting (Manage burst limit, API throttling, IP based blocking, API queue)
 - Service discovery (location of registered microservice - IP and port)
 - Request/response transformation
 - Response caching 
 - Logging

Client request -> AWS Route 53 DNS -> API gateway  

CQRS Pattern (Command Query Responsibility Segregation)

CAP Theorem (Consistency, Availability, Partition tolerance)
 For applications where Consistency is crucial (e.g., banking, finance, ticket booking), a CP system is more appropriate. Availability might be sacrificed temporarily to ensure data accuracy. 
 For applications where Availability is more critical than strict data accuracy (e.g., social media feeds, online shopping), an AP system is usually chosen, allowing temporary inconsistencies that can later be corrected.

SQL - scale vertically, useful when (C)onsistency is important
NoSQL - scale horizontally, useful when (A)vailability is important with fast response time   

DB Replication - high availability, disaster recovery, fault tolerance 
- Master/slave: master receives write operations (insert, update, delete) and changes are replicated to the slave DB
- Master/master: change to 1 master is synched to other master. In case of conflicting writes, conflict resoluton mechanisms are needed to ensure data consistency.
- Snapsnot replication: create a copy of DB at any specific point in time and then replicate it on one or more servers.
- Transactional replication: any change to one DB table (publisher) is immediately replicated to other DB (subscribers)

Synchronous replication 
- helps in Data consistency, failover capability, data loss tolerance 
- causes latency due to waiting for acknowledgement

Async replication 
 - improves performance with low latency at the cost of consistency

=====================================================================================================

RADIO Concept

1) Requirements (functional and non-functional)
2) Architecture/HLD
3) Data model 
4) Interface definition (API)
5) Optimization

https://www.greatfrontend.com/system-design/framework

API Rate limiting strategies:
1. Token Bucket - allows for burst traffic by saving tokens 
   Eg. video streaming, large uploads
   Scenario: Ecommerce app launches a sale. Normally it receives 100 req/sec but during sales, it receives 1000/sec.
   Allow a short burst of up to 500 req/sec (using saved tokens).
   Once the burst capacity is exhausted, new req are queued or rate-limited.
2. Leaky Bucket - rate limiting for APIs where requests are served at a steady, fixed rate regarless of traffic spikes (Excess requests are dropped).
   Scenario. set up API to handle 100 req per sec.
   If 150 requests are made, the remaining 50 are dropped.

System design blog: https://www.eraser.io/decision-node

====================================================================

PROBLEM STATEMENT 1: How to handle sudden high traffic? Eg. Flipkart Sale start at 10am

ASG takes time to horizontally scale and spin up new instances, do health checks and get it ready (1-2 mins).
Traffic during these 1-2 mins are not handled and also increases the bill.

Better Solution: To build a highly scalable system, offload non-critical async tasks to Quueue

Critical tasks - place order (handle synchronously)
Non-critical tasks - send email (handle asyncronously)

Message Queue (Amazon SQS, Kafka) <- Consumer 
(Consumer pulls messsages. Set up rate limiter and concurrency control)

Scenario: User purchases a course and gets email notification on successful payment.
=> Problems
    1. Slow response time 
    2. Server may crash in case of large no. of concurrent requests 
    3. Email service may get blocked eg. 5,000 req received at once 

   Solution: Create separate Node app for queue (SQS/BullMQ/Kafka).

   BullMQ -> Producer, Consumer (Worker)

   Main app - producer code to save data in Queue 
   Queue app - consumer to read data from Queue and send emails 
   (Add limiter eg. max 50 emails per sec)

   Save data in Redis hosted in Aiven.

=========================================================================================================================

PROBLEM STATEMENT 2: 
2a: In an image sharing platform, implement a search feature where we can search by name, username and it returns a page with all images posted by the user
User Service -> User DB (contains millions of records)

Solution: Have a dedicated Search Service -> Search DB 
Event driven architecture - publish data from User service to a message broker. Search service subscribes to it. This keeps user data in sync.

2b: In newsfeed, show posts of all users which are followed by current logged in user.
Solution:  User A followes User B and User C.
user_id  |  timeline - sorted list of posts
---------------------------------------------
user_a   |  userb_post1, userc_post1, userb_post2, userc_post2

=> Posts service -> produce event (img_url, user_id, timestamp) when User B, C posts an image
=> Timeline service -> consume the event asynchronously (new_post -> newly created post)
   Subscribed data (img_url, user_id, timestamp)
   Send request to UserService to get followers of current logged in user 
   user_a -> user_b, userc (follower ids)
   
   user_b  [new_post, post1, post2 ... post5]
   user_c  [new_post, post3, post10,...]
   user_f [post7, post 8...]  // not updated as this user is not a follower

CQRS Pattern (Command Query Responsibility Segregation)
 - Command: user registration/updatest to user service
 - Query: search operations go to Search Service 

===============================

Choosing the right scaling 

1. If app is small, choose vertical scaling 
   - increases capacity of single server like RAM, CPU core, storage capacity
   - simpler and requires fewer changes to architecture compared to horizontal scaling 
   - suitable for small scale users that don't need to handle a lot of concurrent users 

2. If you expect a high increase in user traffic, choose horizontal scaling
   - adds more servers as user base grows 
   - effective for microservice, allowing to scale each service independently 
   
3. For variable loads, choose serverless 
    - automatically manages the scaling of resources based on demand 
    - only pay for the actual resources used, making it cost effective 
    - AWS Lambda handles underlying infrasructure reducing the infrastructure overhead 

4. Low latency and high performance - choose horizontal scaling 
    - Load balancers can help distribute the requests 
    

How does horizontal scaling work in microservice architecture? By scaling only specific service? 
Serverless vs ASG?
Low latency and high performance - ALB v/s API gateway?
How to check the scalalibility of app? 

AWS Microservice whitepaper: https://docs.aws.amazon.com/pdfs/whitepapers/latest/microservices-on-aws/microservices-on-aws.pdf









