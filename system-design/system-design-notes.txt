System design tutorial: https://www.geeksforgeeks.org/system-design-tutorial/

Functional requirements (What the system should do to meet user needs - user authentication, data processing, user actions)
Non-functional requirements (How the system should perform - scalability, security, response time, reliability)

Components of System design:
 - Load balancer (ELB)
 - Caching (Redis, Memcache)
 - CDN (Cloudfront)
 - API Gateway (AWS API gateway)
 - Key value store (AWS DynamoDB)
 - Blob (Object) storage and database (AWS S3)
 - Rate limiter (AWS API gateway throttling)
 - Monitoring system (CloudWatch)
 - Distributed system messaging queue (SQS)
 - Distributed unique id generator (DynamoDB with UUID generation)
 - Distributed search/Elastic search - AWS Opensearch
 - Distributed logging service (CouldTrail, CloudWatch logs)
 - Distributed task scheduler (SNS)

Horizontal scaling/scale-out (add more servers to handle increased workload)
Vertical scaling/scale up (adding more CPU cores, memory or storage capacity to handle increased workload)

Load balancer - distribute the incoming traffic among servers to provide high availability, efficient server utilization and high performance.

Latency - time taken for a single task to complete (Request time + Response time) 
Throughput - No. of tasks completed in a given time period (High throughput usually means low latency)

Improve latency:
 - Deploy services closed to users (using CDN)
 - Load balancer and horizontal scaling
 - Optimize DB queries and index DB tables
 - Use caching Redis/Memcache
 - Async processing (handle non-critical tasks in the background eg. email notification in Amazon SQS)
 - DB sharding

Improve throughput: All above works, in addition to these: 
- Batch processing (Eg. instead of updating 100 transactions, 1 single query is fired instead of 100)
- Replication (use DB replica for read-heavy operations)

Database Sharding (eg. 4M rows -> 4 shards with 1M rows each)
- horizontal scaling of DB where data is split across multiple DB instances or shards 
Types of sharding:
 - Key based sharding (serverIndex = hashValue % numOfServers)
 - Range based sharding (customer name A-P in shard 1 and the rest in shard 2)
 - Vertical sharding (split multiple columns in single table to multiple tables)
    Eg. users (id, email, name, created_at) -> users (id, email, name) user_shard2 (user_id, created_at)
 - Directory based sharding (maintain lookup table for each shard)

Serverless - if traffic goes up and down a lot, serverless is cost-efficient by using resources only when needed

Event streaming (High volume real time data streams eg. Amazon Kinesis)
Message broker (Communication b/w distributed system producers and consumers eg. Amazon SQS)

API gateway
 - API composition (this layer fetches and combines data from multiple microservices 
   eg. when product details is called, data is aggregated from product service, review service, price service)
 - Authentication (Identity provider)
 - Rate limiting (Manage burst limit, API throttling, IP based blocking, API queue)
 - Service discovery (location of registered microservice - IP and port)
 - Request/response transformation
 - Response caching 
 - Logging

Client request -> AWS Route 53 DNS -> API gateway  

CQRS Pattern (Command Query Responsibility Segregation)

CAP Theorem (Consistency, Availability, Partition tolerance)
 (P)artition tolerance is the most important
 For applications where Consistency is crucial (e.g., banking, finance, ticket booking), a CP system is more appropriate. Availability might be sacrificed temporarily to ensure data accuracy. 
 For applications where Availability is more critical than strict data accuracy (e.g., social media feeds), an AP system is usually chosen, allowing temporary inconsistencies that can later be corrected.
 For Ecommerce, we can use a hybrid approach:
 - product browsing (AP)
 - checkout and payment (CP)

Consistency strategies:
   -> Design Pattern (single source of truth, versioning, transaction management)
   -> Consistency models
      - strongly consistency (sync eg. SQl DB with single master node and multiple replicas)
      - eventual consistency (async eg. DynamoDB - initally stored locally in single node, then asynchronously propagated to other nodes)
      - weak consistency (no guarantee when or if replicas will converge eg. Redis or memcache)
        Weak consistency enables high performance and scalability for caching frequently accessed data.
   -> Conflict resolution techniques (Last writer wins, merge strategies)

Thread - lightweight as each thread in a process shares code, data and memory.
it takes less time to create and do context switching

Concurrency v/s Parallelism
 - Concurrency eg. single threaded Node.js application 
   (receives request from user A and makes DB call. While waiting for the DB response, it processes user B request) 
 - Parallelism eg. server CPU with 4 cores runs 4 threads, each thread handling separate user request 

SQL - scale vertically, useful when (C)onsistency is important
NoSQL - scale horizontally, useful when (A)vailability is important with fast response time   

DB Replication - high availability, disaster recovery, fault tolerance 
- Master/slave: master receives write operations (insert, update, delete) and changes are replicated to the slave DB
- Master/master: change to 1 master is synched to other master. In case of conflicting writes, conflict resoluton mechanisms are needed to ensure data consistency.
- Snapsnot replication: create a copy of DB at any specific point in time and then replicate it on one or more servers.
- Transactional replication: any change to one DB table (publisher) is immediately replicated to other DB (subscribers)

Synchronous replication 
- helps in Data consistency, failover capability, data loss tolerance 
- causes latency due to waiting for acknowledgement

Async replication 
 - improves performance with low latency at the cost of consistency

High level design roadmap
 - Capacity estimation (predicting resources like CPU, memory, storage to meet the expected workload)
 - HTTP methods - sockets, polling
 - Server-sent events (one way eg. real time streaming)
 - Resiliency - through replication, redundancy and availability 
 - Paging (divide large datasets into smaller manageable sets)
 - Logging (used to monitor application flow)

CDN:
 - when user requests a content to a CDN, the CDN identifies user's location and routes the request to the nearest edge server 
 - Edge server contains the cached copies of the content, quickly delivers the response
 - Since edge servers are distributed globally, it reduces latency and decreases load time
 - CDN also helps to offload traffic from the main server 
Eg. Cloudflare, Amazon Cloudfront

Cache
 - Application server cache (each server has its own cache, when LB routes to diff server it's unaware of the same data being cached on another server)
 - Distributed cache (data is cached across multiple nodes in a cluster eg. Redis, Memcache)
 - Global cache (single cache space used by multiple nodes eg. Cloudfront)
Caching Types 
 - Webpage caching 
 - DB caching 
 - CDN 
 - Session caching 
 - API response caching

Message Queue - addresses a no. of challenges:
 - asyncronous communication 
 - decoupling (decouples apps from each other, making them independent)

Dead letter Queue - handle messages that cannot be processed successfully 

Scaling Message Queues:
 - Distributed system (A queue that spans multiple nodes, enables horizontal scalability
   Messages are load balanced across all consumers)
 - Partition (A topic user_action is distributed across 3 partitions - user_id%2 - P1, user_id % 2 !=0 P2
   each parition can be consumed independently improving parallelism)

A system can handle thousands of requests by:
 - distributing the workload using multiple brokers 
 - processing orders in parallel using partitions 

Forward Proxy 
 - acts on behalf of the client to send request to server 
 - hides client's identity, used for security, content filtering, bypassing restrictions 
 - Eg. VPN used in companies to restrict access to certain sites 

Reverse Proxy 
 - acts on behald of the server
 - hides server identity (eg. IP address)
 - Used for load balancing, caching, security 
 - Scenario: Ecommerce site uses multipleservers to handle traffic. Reverse proxy sends requests to different servers
 - Eg. AWS ELB forwards incoming traffic to multiple EC2 instances 
       Nginx uses proxy_pass directive to forward request to one of the servers using load balancing strategy.

=====================================================================================================

RADIO Concept

1) Requirements (functional and non-functional)
2) Architecture/HLD
3) Data model 
4) Interface definition (API)
5) Optimization

https://www.greatfrontend.com/system-design/framework

API Rate limiting strategies:
1. Token Bucket - allows for burst traffic by saving tokens 
   Eg. video streaming, large uploads
   Scenario: Ecommerce app launches a sale. Normally it receives 100 req/sec but during sales, it receives 1000/sec.
   Allow a short burst of up to 500 req/sec (using saved tokens).
   Once the burst capacity is exhausted, new req are queued or rate-limited.
2. Leaky Bucket - rate limiting for APIs where requests are served at a steady, fixed rate regarless of traffic spikes (Excess requests are dropped).
   Scenario. set up API to handle 100 req per sec.
   If 150 requests are made, the remaining 50 are dropped.

System design blog: https://www.eraser.io/decision-node

====================================================================

PROBLEM STATEMENT 1: How to handle sudden high traffic? Eg. Flipkart Sale start at 10am

ASG takes time to horizontally scale and spin up new instances, do health checks and get it ready (1-2 mins).
Traffic during these 1-2 mins are not handled and also increases the bill.

Better Solution: To build a highly scalable system, offload non-critical async tasks to Quueue

Critical tasks - place order (handle synchronously)
Non-critical tasks - send email (handle asyncronously)

Message Queue (Amazon SQS, Kafka) <- Consumer 
(Consumer pulls messsages. Set up rate limiter and concurrency control)

Scenario: User purchases a course and gets email notification on successful payment.
=> Problems
    1. Slow response time 
    2. Server may crash in case of large no. of concurrent requests 
    3. Email service may get blocked eg. 5,000 req received at once 

   Solution: Create separate Node app for queue (SQS/BullMQ/Kafka).

   BullMQ -> Producer, Consumer (Worker)

   Main app - producer code to save data in Queue 
   Queue app - consumer to read data from Queue and send emails 
   (Add limiter eg. max 50 emails per sec)

   Save data in Redis hosted in Aiven.

=========================================================================================================================

PROBLEM STATEMENT 2: 
2a: In an image sharing platform, implement a search feature where we can search by name, username and it returns a page with all images posted by the user
User Service -> User DB (contains millions of records)

Solution: Have a dedicated Search Service -> Search DB 
Event driven architecture - publish data from User service to a message broker. Search service subscribes to it. This keeps user data in sync.

2b: In newsfeed, show posts of all users which are followed by current logged in user.
Solution:  User A followes User B and User C.
user_id  |  timeline - sorted list of posts
---------------------------------------------
user_a   |  userb_post1, userc_post1, userb_post2, userc_post2

=> Posts service -> produce event (img_url, user_id, timestamp) when User B, C posts an image
=> Timeline service -> consume the event asynchronously (new_post -> newly created post)
   Subscribed data (img_url, user_id, timestamp)
   Send request to UserService to get followers of current logged in user 
   user_a -> user_b, userc (follower ids)
   
   user_b  [new_post, post1, post2 ... post5]
   user_c  [new_post, post3, post10,...]
   user_f [post7, post 8...]  // not updated as this user is not a follower

CQRS Pattern (Command Query Responsibility Segregation)
 - Command: user registration/updatest to user service
 - Query: search operations go to Search Service 

===============================

Choosing the right scaling 

1. If app is small, choose vertical scaling 
   - increases capacity of single server like RAM, CPU core, storage capacity
   - simpler and requires fewer changes to architecture compared to horizontal scaling 
   - suitable for small scale users that don't need to handle a lot of concurrent users 

2. If you expect a high increase in user traffic, choose horizontal scaling
   - adds more servers as user base grows 
   - effective for microservice, allowing to scale each service independently 
   
3. For variable loads, choose serverless 
    - automatically manages the scaling of resources based on demand 
    - only pay for the actual resources used, making it cost effective 
    - AWS Lambda handles underlying infrasructure reducing the infrastructure overhead 

4. Low latency and high performance - choose horizontal scaling 
    - Load balancers can help distribute the requests 
    

How does horizontal scaling work in microservice architecture? By scaling only specific service? 
Serverless vs ASG?
Low latency and high performance - ALB v/s API gateway?
How to check the scalalibility of app? 

AWS Microservice whitepaper: https://docs.aws.amazon.com/pdfs/whitepapers/latest/microservices-on-aws/microservices-on-aws.pdf

=========================

Some real world examples of DSA in System design:

1. Trie for autocomplete - Tree like data structure used to efficiently store and retrieve keys. 
   Useful for prefix based search. 
2. Hash table for caching - key value pair with fast lookup. When a request is made, the key is hashed to compute its storage location allowing rapid access.
   DB query result caching, web page caching (URL key to HTML value), DNS caching (domain name to IP address)
3. Graphs for social network - network of friends can be represented using a graph. 
   Using graph traversal, identify mutual friends or suggest connections based on shared interests. 
4. Binary search - retrieving a user profile by id 
5. Segment trees - is a binary tree used for range based queries and update in an array 
   Eg. efficiently calculate revenue within a time range 










