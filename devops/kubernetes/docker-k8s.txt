Kubernetes - container orchestration system for managing containerized workloads and services (allows communication among different containers eg. microservices)

https://www.youtube.com/watch?v=X48VuDVv0do

Cluster - set of 1 master and multiple worker nodes 

Need for container orchestration tool - In Microservices, with increased usage of containers, there's a demand for a proper way to manage hundreds of containers 
- High availability
- Scalability
- Disaster recovery 
____________________________________________________________

KUBERNETES COMPONENTS:

1. Pod: Smallest unit of K8s, abstraction over container (layer on top of container). Usually 1 application per pod. 
Each pod gets its own IP address.
New IP address on re-creation.

2. Service: Permanent IP address. Life cycle of pod and service are not connected (even if pod dies, service isn't affected). Also acts as load balancer. Define blueprint for pods to create pod replica.

3. Ingress - request comes to ingress first, then forwarded to external service. Ingress has url like https://my-ap.com and ext service has url like 127.0.0.1:80

4. ConfigMap: external configuration of your application (Eg. DB credentials)
Insted of building app every time config is changed, fetch from ConfigMap.
Secrets: For DB password 

K8s cluster explicitly doesn't manage data persistence. If DB container or pod is restarted, data will be gone. 
Volumes - attaches physical storage or hard drive to the pod. Storage can be in local (same server node) or remote storage (cloud).

Deployment: blueprint for my-app pods. Abstraction of pods. Not used for DB.

StatefulSet: for stateful apps eg. DB. 
DB are often hosted outside K8s cluster.
____________________________________________________________

WORKER MACHINE IN K8S CLUSTER 

Communication across different nodes is done using services
Each nodes has multiple pods
3 processes must be installed on every node 
    1. Container runtime eg. Docker
    2. Kubelet (interacts with both container and node. Starts the pod with a container inside)
    3. Kube proxy (forwarding request from services to pods - intelligent forwarding logic is present as it forwards to replicate on the same node if present, thus saving overhead)
 
Worker node do the actual work.
Master node helps to manage 

MASTER PROCESS 
4 processes must be in Master node:
    1. API server - interact with API server using some client. It's like a cluster gateway and acts as a gatekeeper for authentication. (Load balancer)
    2. Scheduler - checks worker node resources to decide where to put the new pod.
    Kubelet gets the request.
    3. Controller manager - detects cluster state changes eg. crashing of pods. When pods die, it makes request to scheduler to restart the pods. 
    4. etcd - Key value store to save cluster state info (eg. is the cluster state healthy, what resources are available, did the cluster state change) 
    application data is not stored in etcd.
    Distributed storage across all master nodes

2 Master - need less resources
3 Worker nodes - more resources 

Add new master/node server:
1. Get new bare server
2. Install all master/worker node processes 
3. Add it to the cluster.

Minikube - master and worker processes work on 1 node (test/local setup). Docker pre-installed. Creates virtual box on your machine and node runs in that virtual box.
kubectl - CLI tool to interact with Kubenetes setup.

minikube is used to start and delete cluster. kubectl is used for all other commands.

We can't create pod directly, need to create deployment as it's an abstraction on pod. 

CLI COMMANDS:
    // Get latest nginx docker image
    kubectl create deployment nginx-depl --image=nginx 
    kubectl get deployment 
    kubectl get pod 
    kubectl get replicaset 
    // edit config file generated at the time of creatng the deployment
    kubectl edit deployment nginx-depl 
    // open interactive terminal to debug the pod
    kubectl exec -it <pod-name> -- bin/bash
    // Run create/update command based on yaml file contents
    kubectl apply -f config.yaml
    kubectl describe service service-name
    # Yaml output written to nginx-depl.yaml
    kubectl get deployment <deployment-name> -o yaml > nginx-depl.yaml

Deployment (manages ->) ReplicaSet (manages ->) Pod (abstraction of ->) Container 
Everything below deployment is managed by Kubernetes 
If deployment is edited, everything below is re-created automatically.

Config file:
 - metadata
 - specification 
 - status (added automatically by K8s, keeps changing)

etcd holds the current status of any K8s components 

metadata:
    .....
spec: 
    # template has its own metadata & specs sections
    # applies to pods 
    template:
        metadata: 
            ....
        specs: 
            containers:
            ....

Internal service type is ClusterIP
External service type is LoadBalancer (LB assigns internal + external IP)

# Assign external IP to service
minikube service <service-name> 

Namespace - use cases:
1. Structure your components
2. Avoid conflicts among teams - many teams, same application (same namespace can override existing one)
3. Resource sharing in different env or Blue/Green deployment 
4. Access and resource limits on namespaces
   (Limit CPU, RAM, storage in namespace)

- Each NS should have its own ConfigMap. 
- Some components live globally in the cluster, NS not allowed (Eg. volume, node)

Install kubens - switch active namespace. No need to mention --namespace in CLI every time

# Install ingress controller in minikube
# automatically starts K8s nginx implementation of ingress controller 
minikube addons enable ingress

kubectl get pod -n kube-system

Access K8s dashboard from some domain name 
kubectl get ns 
kubectl get all -n kubernetes-dashboard 

Configuring TLS certificate 
// Secret component 
metadata: 
    name: myaoo-secret-tls 
    namespace: default 
data: 
    tls.crt: base64 encoded crt 
    tls.key: base64 encoded key 

Secret component must be in the same namespace as the ingress component 










